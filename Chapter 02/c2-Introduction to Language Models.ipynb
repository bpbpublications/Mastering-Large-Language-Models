{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d84149",
   "metadata": {},
   "source": [
    "# Training 'Rule-Based Models'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "987b70d2",
   "metadata": {},
   "source": [
    "**Example 1: Using regular expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271bb55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found match: 123-456-7890\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a pattern using a regular expression\n",
    "pattern = r\"\\d{3}-\\d{3}-\\d{4}\"  # Matches phone numbers in the format XXX-XXX-XXXX\n",
    "\n",
    "# Text to be processed\n",
    "text = \"Please call me at 123-456-7890.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907a465",
   "metadata": {},
   "source": [
    "**Example 2: Matching patterns using part-of-speech tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d926fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy playing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern using part-of-speech tags\n",
    "pattern = [{'POS': 'NOUN'}, {'POS': 'VERB'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('NOUN_VERB_NOUN_PATTERN', [pattern])\n",
    "\n",
    "# Text to be processed\n",
    "text = \"I saw a boy playing in the gardan.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply the matcher to the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633dbce3",
   "metadata": {},
   "source": [
    "**Example 3: Matching specific tokens with specific attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f31a367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern\n",
    "pattern = [{'LOWER': 'machine'}, {'LOWER': 'learning'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('ML_PATTERN', [pattern])\n",
    "\n",
    "# Text to be processed\n",
    "text = \"I am interested in machine learning and deep learning.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply the matcher to the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b80c49",
   "metadata": {},
   "source": [
    "# Training 'Statistical Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c84e9c",
   "metadata": {},
   "source": [
    "**Example 4: simple implementation of a bigram language model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762071e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'house', 'This', 'is', 'a', 'home', 'I', 'love', 'my', 'house', 'This', 'is', 'my', 'home', 'Is', 'this', 'your', 'house?']\n",
      "\n",
      " All the possible Bigrams are \n",
      "[('This', 'is'), ('is', 'a'), ('a', 'house'), ('This', 'is'), ('is', 'a'), ('a', 'home'), ('I', 'love'), ('love', 'my'), ('my', 'house'), ('This', 'is'), ('is', 'my'), ('my', 'home'), ('Is', 'this'), ('this', 'your'), ('your', 'house?')]\n",
      "\n",
      " Bigrams along with their frequency \n",
      "{('This', 'is'): 3, ('is', 'a'): 2, ('a', 'house'): 1, ('a', 'home'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'house'): 1, ('is', 'my'): 1, ('my', 'home'): 1, ('Is', 'this'): 1, ('this', 'your'): 1, ('your', 'house?'): 1}\n",
      "\n",
      " Unigrams along with their frequency \n",
      "{'This': 3, 'is': 3, 'a': 2, 'house': 2, 'home': 2, 'I': 1, 'love': 1, 'my': 2, 'Is': 1, 'this': 1, 'your': 1}\n",
      "\n",
      " Bigrams along with their probability \n",
      "{('This', 'is'): 1.0, ('is', 'a'): 0.6666666666666666, ('a', 'house'): 0.5, ('a', 'home'): 0.5, ('I', 'love'): 1.0, ('love', 'my'): 1.0, ('my', 'house'): 0.5, ('is', 'my'): 0.3333333333333333, ('my', 'home'): 0.5, ('Is', 'this'): 1.0, ('this', 'your'): 1.0, ('your', 'house?'): 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Define a function to read the data from a list of sentences\n",
    "def readData():\n",
    "    # Initialize an empty list to store the words\n",
    "    data = ['This is a house','This is a home','I love my house','This is my home', 'Is this your house?']\n",
    "    dat=[]\n",
    "    # Loop through each sentence in the data\n",
    "    for i in range(len(data)):\n",
    "        # Split the sentence into words and append them to the list\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    # Print the list of words\n",
    "    print(dat)\n",
    "    # Return the list of words\n",
    "    return dat\n",
    "\n",
    "# Define a function to create bigrams from the list of words\n",
    "def createBigram(data):\n",
    "   # Initialize an empty list to store the bigrams\n",
    "   listOfBigrams = []\n",
    "   # Initialize an empty dictionary to store the bigram counts\n",
    "   bigramCounts = {}\n",
    "   # Initialize an empty dictionary to store the unigram counts\n",
    "   unigramCounts = {}\n",
    "   # Loop through each word in the data except the last one\n",
    "   for i in range(len(data)-1):\n",
    "      # Check if the next word is lowercase (to avoid punctuation marks)\n",
    "      if i < len(data) - 1 and data[i+1].islower():\n",
    "         # Create a bigram tuple from the current and next word and append it to the list\n",
    "         listOfBigrams.append((data[i], data[i + 1]))\n",
    "         # Increment the count of the bigram in the dictionary or set it to 1 if not present\n",
    "         if (data[i], data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i], data[i + 1])] += 1\n",
    "         else:\n",
    "            bigramCounts[(data[i], data[i + 1])] = 1\n",
    "      # Increment the count of the current word in the dictionary or set it to 1 if not present\n",
    "      if data[i] in unigramCounts:\n",
    "         unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "         unigramCounts[data[i]] = 1\n",
    "   # Return the list of bigrams, the unigram counts and the bigram counts\n",
    "   return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "# Define a function to calculate the bigram probabilities from the counts\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    # Initialize an empty dictionary to store the bigram probabilities\n",
    "    listOfProb = {}\n",
    "    # Loop through each bigram in the list\n",
    "    for bigram in listOfBigrams:\n",
    "        # Get the first and second word of the bigram\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        # Calculate the probability of the bigram as the ratio of its count and the count of the first word\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    # Return the dictionary of bigram probabilities\n",
    "    return listOfProb\n",
    "\n",
    "\n",
    "# Call the readData function and store the result in data variable\n",
    "data = readData()\n",
    "# Call the createBigram function with data as argument and store the results in three variables\n",
    "listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
    "\n",
    "# Print some messages and results for debugging purposes\n",
    "print(\"\\n All the possible Bigrams are \")\n",
    "print(listOfBigrams)\n",
    "\n",
    "print(\"\\n Bigrams along with their frequency \")\n",
    "print(bigramCounts)\n",
    "\n",
    "print(\"\\n Unigrams along with their frequency \")\n",
    "print(unigramCounts)\n",
    "\n",
    "# Call the calcBigramProb function with the counts as arguments and store the result in bigramProb variable\n",
    "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "print(\"\\n Bigrams along with their probability \")\n",
    "print(bigramProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53473bbe",
   "metadata": {},
   "source": [
    "**Example 5: Example of find ngrams of the sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cff8a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram of the sample text:  ['I', 'am', 'interested', 'in', 'machine', 'learning', 'and', 'deep', 'learning', '.'] \n",
      "\n",
      "2-gram of the sample text:  ['I am', 'am interested', 'interested in', 'in machine', 'machine learning', 'learning and', 'and deep', 'deep learning', 'learning .'] \n",
      "\n",
      "3-gram of the sample text:  ['I am interested', 'am interested in', 'interested in machine', 'in machine learning', 'machine learning and', 'learning and deep', 'and deep learning', 'deep learning .'] \n",
      "\n",
      "4-gram of the sample text:  ['I am interested in', 'am interested in machine', 'interested in machine learning', 'in machine learning and', 'machine learning and deep', 'learning and deep learning', 'and deep learning .'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "My_text = 'I am interested in machine learning and deep learning.'\n",
    "\n",
    "print(\"1-gram of the sample text: \", extract_ngrams(My_text, 1), '\\n')\n",
    "print(\"2-gram of the sample text: \", extract_ngrams(My_text, 2), '\\n')\n",
    "print(\"3-gram of the sample text: \", extract_ngrams(My_text, 3), '\\n')\n",
    "print(\"4-gram of the sample text: \", extract_ngrams(My_text, 4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef915dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
